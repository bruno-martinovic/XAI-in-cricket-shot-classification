{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29fa0ff-392d-44d4-a7c5-3f6ffcffd6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python mediapipe numpy tqdm ultralytics --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b07219a-6806-4f9f-9c2a-5ecab5e3d8b3",
   "metadata": {},
   "source": [
    "## Import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5edb1fd8-456e-46d5-8bf3-40786413db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Pose and YOLOv8 model\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False)\n",
    "yolo = YOLO(\"yolov8n.pt\")  # You can use yolov8s.pt for higher accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9e742-252e-49f7-81a2-7e513b479c7e",
   "metadata": {},
   "source": [
    "## Trim Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a0ff23-f0b6-4d0e-ad17-2372405a0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function trims the video by 0.5 seconds at the beginning and 1 second at the end.\n",
    "# Values can be altered through the trim_start_sec and trim_end_sec respectively.\n",
    "def trim_video(input_path, output_path, trim_start_sec=0.5, trim_end_sec=1.0):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Determine which frames to keep\n",
    "    start_frame = int(trim_start_sec * fps)\n",
    "    end_frame = total_frames - int(trim_end_sec * fps)\n",
    "\n",
    "    # Setup video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Write only the selected frames\n",
    "    frame_idx = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if start_frame <= frame_idx < end_frame:\n",
    "            out.write(frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c1cf2d-467a-4948-a817-fdcc6e8a402c",
   "metadata": {},
   "source": [
    "## Get all keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c7e0de-05c0-443d-b3cb-142d7a2cd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_frames_to_csv(input_root, output_csv_path):\n",
    "    rows = []\n",
    "    labels = []\n",
    "    video_entries = []\n",
    "\n",
    "    # Step 1: Collect all .mp4 videos and subfolder names\n",
    "    for root, _, files in os.walk(input_root):\n",
    "        for file in files:\n",
    "            if file.endswith(\".mp4\"):\n",
    "                full_input_path = os.path.join(root, file)\n",
    "                class_label = os.path.basename(root)\n",
    "                relative_subfolder = os.path.relpath(root, input_root)\n",
    "                video_entries.append((full_input_path, class_label, relative_subfolder, file))\n",
    "\n",
    "    # Step 2: Process each video, frame-by-frame\n",
    "    for input_path, label, subfolder, file in tqdm(video_entries, desc=\"Processing videos\"):\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        frame_idx = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_idx += 1\n",
    "\n",
    "            # Detect people\n",
    "            detections = yolo(frame)[0].boxes.data.cpu().numpy()\n",
    "            person_boxes = [d for d in detections if int(d[5]) == 0]\n",
    "            if not person_boxes:\n",
    "                continue\n",
    "\n",
    "            # Use top-most person (smallest y1)\n",
    "            person_boxes.sort(key=lambda b: b[1])\n",
    "            x1, y1, x2, y2, *_ = person_boxes[0]\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "            cropped = frame[y1:y2, x1:x2]\n",
    "            if cropped.size == 0:\n",
    "                continue\n",
    "\n",
    "            cropped_rgb = cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)\n",
    "            result = pose.process(cropped_rgb)\n",
    "\n",
    "            if result.pose_landmarks:\n",
    "                keypoints = []\n",
    "                for lm in result.pose_landmarks.landmark[:17]:\n",
    "                    keypoints.extend([lm.x, lm.y, lm.z])\n",
    "\n",
    "                # Append row: keypoints + metadata\n",
    "                rows.append(keypoints + [label, file, frame_idx])\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "    # Step 3: Save all keypoints to a CSV\n",
    "    if rows:\n",
    "        header = [f'kp_{i}' for i in range(len(rows[0]) - 3)] + ['label', 'video', 'frame']\n",
    "        with open(output_csv_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header)\n",
    "            writer.writerows(rows)\n",
    "        print(f\"✅ Saved {len(rows)} frames to: {output_csv_path}\")\n",
    "    else:\n",
    "        print(\"⚠️ No keypoints extracted. CSV not created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defba090-1a34-4412-a897-c94334b55bd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Provide input video folder with subfolders\n",
    "process_all_frames_to_csv(\"../Processed Videos\", \"keypoints_per_frame.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b228c64-6b5d-4792-b72f-c62fa6fc000e",
   "metadata": {},
   "source": [
    "## Pose extraction using YOLO object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ce5557-35c4-48bd-b9db-8661a8311684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function serves as a check to see if YOLOv8 is taking the correct person.\n",
    "def detect_batter_and_extract_pose(video_path, max_frames=150, show=True):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    pose = mp_pose.Pose(static_image_mode=False)\n",
    "    frame_count = 0\n",
    "    pose_results = []\n",
    "\n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "\n",
    "        # Run YOLO detection\n",
    "        detections = yolo(frame)[0].boxes.data.cpu().numpy()\n",
    "        person_boxes = [d for d in detections if int(d[5]) == 0]  # class 0 = person\n",
    "\n",
    "        if not person_boxes:\n",
    "            continue\n",
    "\n",
    "        # Sort by y1 (top of box) — highest person in frame comes first\n",
    "        person_boxes.sort(key=lambda b: b[1])  # b[1] = y1\n",
    "        x1, y1, x2, y2, conf, cls = person_boxes[0]  # topmost person\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "        cropped = frame[y1:y2, x1:x2]\n",
    "\n",
    "        if cropped.size == 0:\n",
    "            continue\n",
    "\n",
    "        cropped_rgb = cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)\n",
    "        result = pose.process(cropped_rgb)\n",
    "\n",
    "        if result.pose_landmarks:\n",
    "            pose_results.append(result.pose_landmarks)\n",
    "\n",
    "            if show:\n",
    "                annotated = cropped_rgb.copy()\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    annotated,\n",
    "                    result.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS\n",
    "                )\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.imshow(annotated)\n",
    "                plt.title(f\"Batter Pose @ Frame {frame_count}\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "    cap.release()\n",
    "    pose.close()\n",
    "    return pose_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1351f52e-5c00-48ed-9562-b0788ec063af",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_batter_and_extract_pose(\"../CKT Dataset/Cover Drive/(100).mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
